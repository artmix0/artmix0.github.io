name: Scrapuj stronę z JS

on:
  schedule:
    - cron: "0 6 * * *"  # codziennie o 6:00 rano
  workflow_dispatch:      # możliwość ręcznego uruchomienia

permissions:
  contents: write

jobs:
  scrape:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repo
        uses: actions/checkout@v4

      - name: Użyj Node.js
        uses: actions/setup-node@v4
        with:
          node-version: "20"

      - name: Zainstaluj Puppeteer oraz fs
        run: npm install puppeteer fs

      - name: Scrapuj stronę
        run: |
          node <<'EOF'
          import puppeteer from "puppeteer";
          import fs from "fs";

          const url = "https://www.zsk.poznan.pl/plany_lekcji/2025";

          const run = async () => {
            const browser = await puppeteer.launch({
              args: ['--no-sandbox', '--disable-setuid-sandbox']
            });
            const page = await browser.newPage();
            await page.goto(url, { waitUntil: 'networkidle0' });

            const przedmioty = await page.evaluate(() => {
              const elements = Array.from(document.querySelectorAll(".p")).map(el => el.textContent.trim());
              return elements;
            });

            fs.writeFileSync("scraped.json", JSON.stringify(przedmioty, null, 2));
            console.log("Zapisano scraped.json");

            await browser.close();
          };

          run();
          EOF

      - name: Commit i push zmian
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          git add scraped.json
          git commit -m "Automatyczna aktualizacja danych" || echo "Brak zmian"
          git push
