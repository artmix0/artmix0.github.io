name: Scrapuj stronę z JS

on:
  schedule:
    - cron: "0 6 * * *"  # codziennie o 6:00 rano
  workflow_dispatch:      # możliwość ręcznego uruchomienia

permissions:
  contents: write

jobs:
  scrape:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repo
        uses: actions/checkout@v4

      - name: Użyj Node.js
        uses: actions/setup-node@v4
        with:
          node-version: "20"

      - name: Zainstaluj Puppeteer oraz fs
        run: npm install puppeteer fs

      - name: Scrapuj stronę
        run: |
          node <<'EOF'
          import puppeteer from "puppeteer";
          import fs from "fs";
      
          const url = "https://www.zsk.poznan.pl/plany_lekcji/2025";
      
          const run = async () => {
            const browser = await puppeteer.launch({
              args: ['--no-sandbox', '--disable-setuid-sandbox']
            });
            const page = await browser.newPage();
            await page.goto(url, { waitUntil: 'networkidle0' });
      
            // pobierz wszystkie ramki
            const frames = page.frames();
            const planFrame = frames.find(f => f.name() === 'plan');
      
            // poczekaj aż ramka się w pełni wyrenderuje
            await new Promise(resolve => setTimeout(resolve, 5000)); 
      
            // wyciągnij HTML planu
            const planHTML = await planFrame.evaluate(() => document.body.innerHTML);
      
            // zapisz do pliku JSON (lub HTML jeśli wolisz)
            fs.writeFileSync('scraped.json', JSON.stringify({ html: planHTML }, null, 2));
      
            await browser.close();
            console.log("Zapisano scraped.json");
          };
      
          run();
          EOF

      - name: Commit i push zmian
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          git add scraped.json
          git commit -m "Automatyczna aktualizacja danych" || echo "Brak zmian"
          git push
